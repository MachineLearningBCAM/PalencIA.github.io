<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>PalencIA - Location</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="about.html">Location</a>
                    </li>
                    <li>
                        <a href="schedule.html">Schedule</a>
		    </li>
                                      <li>
                        <a href="organizers.html">Contact organizers</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('workshop.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1>PalencIA 2024</h1>
                        <hr class="small">
                        <span class="subheading">Schedule</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
		            <div class="col-lg-20 col-lg-offset-2 col-md-20 col-md-offset-1">
               
             <p> 
<pre>
			    <table class="center" style="width:100%">
			<tr>
			<th>Day</th>
			<td colspan="4" rowspan="1" class="stage-saturn" style="font-size: 30px;">Wednesday</td>
			<td colspan="4" rowspan="1" class="stage-saturn" style="font-size: 30px;">Thursday</td>
			<td colspan="4" rowspan="1" class="stage-saturn" style="font-size: 30px;">Friday</td>
		</tr>
		<tr>
			<th>08:00</th>
			<td colspan="4" rowspan="2"> </td>
			<td colspan="4" rowspan="2" class="stage-earth" style="font-size: 30px;">Breakfast</td>
			<td colspan="4" rowspan="2" class="stage-earth" style="font-size: 30px;">Breakfast</td>
		</tr>
		<tr>
			<th>08:30</th>
		</tr>
		<tr>
			<th>09:00</th>
			<td colspan="4" rowspan="4"> </td>
			<td colspan="4" rowspan="4" class="stage-mars" style="font-size: 15px;">Talk</td>
			<td colspan="4" rowspan="4" class="stage-venus" style="font-size: 15px;">Iñigo A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference
 Xabier de Juan Optimality of the Median-of-Means for Learning Under Contamination
Mario Martinez Enhancing model performance through privileged information: A preliminary approach with probability distributions to harness privileged features Aitor Self-Supervised Anomaly Detection in Time Series: A Brief Introduction.
</td>
		</tr>
		<tr>
			<th>09:30</th>
		</tr>
		<tr>
			<th>10:00</th>
		</tr>
		<tr>
			<th>10:30</th>
		</tr>
		<tr>
			<th>11:00</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Break</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Break</td>
		</tr>
		<tr>
			<th>11:30</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1" class="stage-venus" style="font-size: 15px;">Andoni Irazusta Leveraging Graph Neural Networks for Combinatorial Optimization
 Mikel Malagon Self-Composing Policies for Scalable Continual Reinforcement Learning</td>
			<td colspan="4" rowspan="1" class="stage-jupiter" style="font-size: 15px;">Talks <span>Mars Stage</span></td>
		</tr>
		<tr>
			<th>12:00</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1" class="stage-venus">Talks <span>Venus Stage</span></td>
			<td colspan="4" rowspan="1" class="stage-jupiter">Talks <span>Mars Stage</span></td>
		</tr>
		<tr>
			<th>12:30</th>
			<td colspan="4" rowspan="2"> </td>
			<td colspan="4" rowspan="2" class="stage-venus" style="font-size: 15px;">Josu <span>Time to stop and think: what kind of research do we want to do?</span></td>
			<td colspan="4" rowspan="2" class="stage-jupiter"style="font-size: 15px;">Talks <span>Mars Stage</span></td>
		</tr>
		<tr>
			<th>13:00</th>
		</tr>
		<tr>
			<th>13:30</th>			
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Welcome</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Bus to Cervera</td>
			<td colspan="4" rowspan="4" class="stage-earth" style="font-size: 30px;">Lunch break</td>
		</tr>
		<tr>
			<th>14:00</th>
			<td colspan="4" rowspan="4" class="stage-earth" style="font-size: 30px;">Lunch break</td>
			<td colspan="4" rowspan="4" class="stage-earth" style="font-size: 30px;">Lunch break</td>
		</tr>
		<tr>
			<th>14:30</th>
		</tr>
		<tr>
			<th>15:00</th>
		</tr>
		<tr>
			<th>15:30</th>
		</tr>
		<tr>
			<th>16:00</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Check in</td>
			<td colspan="4" rowspan="3" class="stage-saturn" style="font-size: 30px;">Talk</td>
			<td colspan="4" rowspan="3"> </td>
		</tr>
		<tr>
			<th>16:30</th>
			<td colspan="4" rowspan="2" class="stage-saturn" style="font-size: 15px;">Santiago Mazuelas  <span>Minimax Semi-Supervised Learning with Performance Guarantees</span>
			Verónica Álvarez <span>Weak supervision</span> Nicolás Errandonea <span>Graph based ML models for the partially labeled classification problem</span></td>
		</tr>
		<tr>
			<th>17:00</th>
		</tr>
		<tr>
			<th>17:30</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Break</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 30px;">Break</td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>18:00</th>
			<td colspan="4" rowspan="3" class="stage-saturn" style="font-size: 15px;">Onintze Zaballa<span>Multi-output</span> Paula Martín<span>Machine Learning for prediction in energy markets</span>Fernando García<span>Cost-sensitive ordinal classification methods to predict SARS-CoV-2 pneumonia severity
intelligence machine learning (AI-ML) strategies in the prognosis of SARS-CoV-2 pneumonia severity.</span</td>
			<td colspan="4" rowspan="3" class="stage-saturn" style="font-size: 30px;">Talk</td>
			<td colspan="4" rowspan="3"> </td>
		</tr>
		<tr>
			<th>18:30</th>
		</tr>
		<tr>
			<th>19:00</th>
		</tr>
		<tr>
			<th>19:30</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Bus to Cervera</td>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>20:00</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>20:30</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Dinner <span>Location: Cervera</span></td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Dinner <span>Location: Hotel</span></td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
      
</table>
</pre>
                </p>
            </div>
        </div>
    </div>

    <hr>

	    <div class="container">
        <div class="row">
                           <div class="col-lg-20 col-lg-offset-2 col-md-20 col-md-offset-1">
             <p> 
<h1 style="color:blue;" >Abstracts </h1>
<h2 style="color:red;">Santiago Mazuelas</h2>
<h3>Minimax Semi-Supervised Learning with Performance Guarantees</h3>

Abstract: Semi-supervised learning holds promise to significantly increase the cost-efficiency of data acquisition by using a relatively small set of supervised samples. The scarcity of labeled samples poses important methodological challenges, for instance, the limited availability of validation samples hinders the usage of conventional tools to assess predictive performance. Both in theory and practice, the benefits of semi-supervision have been manifested in certain specific situations, in contrast to the broad success of supervised and unsupervised methods. This talk presents semi-supervised minimax risk classifiers (SMRCs) that can leverage the representation and discriminative capabilities of general supervised and unsupervised learning algorithms. In addition, we provide generalization bounds for SMRCs, show how the methods proposed can provide computable performance guarantees, and present efficient learning algorithms based on stochastic subgradients. The experimental results show that SMRCs can effectively leverage general algorithms and provide practically useful performance guarantees.

<h2 style="color:red;">Onintze Zaballa</h2>
		     <h3></h3>
<h2 style="color:red;">Verónica Álvarez</h2>
		     <h3></h3>
<h2 style="color:red;">Nicolás Errandonea</h2>
		     <h3>Graph based ML models for the partially labeled classification problem</h3>
		     Graph based machine learning models have been broadly analyzed and used in the semi-supervised setting. However, the application of graph models to other less explored weakly supervised settings is yet to be explored. In the partially labeled problem,  instances are paired with sets of possible labels that contain the true label.The present work analyzes the possible advantages of applying graph based  models to the partially labeled problem, since the elaboration of a  model  that actively exploits  the underlying connections among the data elements presents itself as a promising  line of research.

<h2 style="color:red;">Xabier de Juan Soriano</h2>
		     <h3>Optimality of the Median-of-Means for Learning Under Contamination
</h3>
		     We present a concentration inequality for the Median-of-Means (MoM) estimator under adversarial corruption which achieves the minimax optimal order under minimal conditions. In addition, we show that the derived bound can be used to obtain generalization bounds for minimax classification rules under adversarial contamination
<h2 style="color:red;">José Ignacio Segovia</h2>
		     <h3>Double-Weighting Adaptation for Multi-Source Covariate Shift
</h3>
		     In multi-source covariate shift scenarios, the training data is obtained from multiple sources, each of which has different marginal probability distributions over the instances. Most of the existing approaches extend the reweighted covariate shift approach to the multi-source scenario, leading to two-stage weighting methods. Other approaches combine classification rules learnt independently on each source. However, existing methods inherit the limitations of reweighted approaches, do not exploit complementary information among sources, and equally combine sources for all instances. We propose a learning methodology, multi-source double-weighting (MS-DW), based on the double-weighting approach for multi-source covariate shift adaptation. The presented methods assign source-dependent weights for training and testing instances, where weights are obtained jointly using information from all sources. Theoretically, we develop generalization bounds for the proposed methods that show a significant improvement in estimation error by leveraging rich complementary information among sources. Empirically, the proposed methods achieve enhanced classification performance in both synthetic and empirical experiments.
<h2 style="color:red;">Iñigo Urteaga</h2>
		     <h3>A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference
</h3>
		     I will present a stochastic variational inference method for Gaussian processes (GPs), which is based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for GPs (CVTGP) is defined in terms of the GP prior and the data-likelihood; hence, accommodating the modeling inductive biases.

CVTGP is amenable to stochastic optimization, it reduces the learnable parameter size, enjoys numerical stability, and maintains state-of-the-art time- and space-complexity.

Results on simulated and real-world regression problems with Gaussian observation noise validate that CVTGP provides better evidence lower-bound estimates and predictive root mean squared error than alternative stochastic GP inference methods.
<h2 style="color:red;">Yuxiao Li</h2>
		     <h3></h3>
<h2 style="color:red;">Ainhize Barrainkua</h2>
		     <h3>Fairness without Demographics through Uncertainty Set Modification
</h3>
		     Minimax risk classifiers optimize worst-case accuracy under distributional uncertainty, while being blind to demographics. This implicitly aligns with the underlying goal of minimax fairness, which seeks a classification rule that maximizes worst-group accuracy. The fairness guarantees of minimax risk classifiers depend on the uncertainty set chosen. We propose a method to determine the optimal uncertainty set size that maximizes worst-group accuracy, with partial or no access to sensitive information.  
<h2 style="color:red;">Regis Konan</h2>
		     <h3>Exploring Autoencoders and Variational Autoencoders: Theory, Applications,
and Future Directions.
</h3>
		     Autoencoders (AEs) and variational autoencoders (VAEs) are fundamental architectures in
machine learning, providing powerful tools for data compression, reconstruction, and generation.
This presentation aims to explore the theory behind these models, their practical implementation,
and future research directions. In the first part, we present the theoretical underpinnings
of AEs and VAEs, highlighting their ability to learn efficient representations of data by reducing
dimensionality while preserving essential information. We also explore the main differences between
the two approaches, in particular the latent distribution constraint imposed by VAEs. The
second part focuses on the practical application of AEs and VAEs, with an in-depth study of the
impact of network architectures on their performance. We examine how different network structures
affect the ability to reconstruct and generate models, thus providing valuable information
for the design of robust machine learning systems. We then consider an in-depth analysis of the
impact of learning and optimization parameters and the impact of the regularization parameter
β on VAE loss and investigate the impact of VAE priors. Finally, in the third part, we discuss
future developments and challenges in this domain.
<h2 style="color:red;">Leo Vattoly</h2>
		     <h3> Early Time Series Classification For Polymer Systems
</h3>
		     My work is centred on the application of Machine Learning (ML) techniques to polymer systems. We are currently in the process of developing algorithms utilizing data obtained from a mathematical model that simulates polymer reactions. This simulator is capable of producing time series data that corresponds to the morphology of polymer particles (structure of polymer particles). As an ML developer, my primary responsibility involves classifying these reactions based on their anticipated morphology. If the reactor conditions fail to produce the expected structure by the conclusion of the reaction, it is imperative to promptly cease the process to prevent the wastage of chemical resources. I am utilizing an Early Time Series Classification (ETSC) methodology to address this issue. Our objective is to minimize the number of timestamps in the data while maximizing the accuracy of the classification model.
<h2 style="color:red;">Kartheek</h2>
		     <h3>Constraint generation method for efficient learning of minimax risk classifiers
</h3>
		     High-dimensional data is common in multiple areas, such as health care and genomics, where the number of features can be hundreds of thousands. In addition, learning methods often perform a high-dimensional representation of the input data vector in order to improve the classification performance such as self supervised approach. In such scenarios, the large number of features lead to inefficient learning. This talk will present methods based on constraint generation to enable efficient learning of minimax risk classifiers (MRCs). In particular, I will discuss methods efficient in high dimensions and its extension to settings with a large number of samples (large number of features in dual). 
<h2 style="color:red;">Xabier Benavides</h2>
		     <h3></h3>
<h2 style="color:red;">Mario Martinez</h2>
		     <h3>Enhancing model performance through privileged information: A preliminary approach with probability distributions to harness privileged features.
</h3>
		     The paradigm of privileged information leverages privileged features present at training time, but not at prediction, as additional information for training the models. A preliminary approach for learning with privileged information is presented. Firstly, a probability distribution for the privileged feature is generated for each instance using a neural network and regular features (available at training and prediction time) as inputs. Secondly, alternatives are proposed to exploit the probability distributions and improve the performance of the model trained with regular features.
<h2 style="color:red;">Rodrigo Gonzalez</h2>
		     <h3></h3>
<h2 style="color:red;">Ioar Casado</h2>
		     <h3></h3>
<h2 style="color:red;">Aritz Perez</h2>
		     <h3></h3>
<h2 style="color:red;">Fernando Garcia</h2>
		     <h3>Cost-sensitive ordinal classification methods to predict SARS-CoV-2 pneumonia severity
intelligence machine learning (AI-ML) strategies in the prognosis of SARS-CoV-2 pneumonia severity.
</h3>
		     Materials & methods: Observational, retrospective, longitudinal, cohort study in 4 hospitals in
Spain. Information regarding demographic and clinical status was supplemented by socioeconomic
data and air pollution exposures.
We proposed AI-ML algorithms for ordinal classification via ordinal decomposition and for costsensitive
learning via resampling techniques. For performance-based model selection, we defined a
custom score including per-class sensitivities and asymmetric misprognosis costs. 260 distinct AIML
models were evaluated via 10 repetitions of 5Å~5 nested cross-validation with hyperparameter
tuning. Model selection was followed by the calibration of predicted probabilities. Final overall
performance was compared against five well-established clinical severity scores and against a
‘standard’ (non-cost sensitive, non-ordinal) AI-ML baseline.
In our best model, we also evaluated its explainability with respect to each of the input variables.
Results: The study enrolled n=1548 patients: 712 experienced low, 238 medium, and 598 high
clinical severity. d=131 variables were collected, becoming d′=148 features after categorical
encoding. Model selection resulted in our best-performing AI-ML pipeline having: a) no imputation
of missing data, b) no feature selection (i.e. using the full set of d′ features), c) ‘Ordered Partitions’
ordinal decomposition, d) cost-based reimbalance, and e) a Histogram-based Gradient Boosting
classifier. This best model (calibrated) obtained a median accuracy of 68.1% [67.3%, 68.8%] (95%
confidence interval), a balanced accuracy of 57.0% [55.6%, 57.9%], and an overall area under the
curve (AUC) 0.802 [0.795, 0.808]. In our dataset, it outperformed all five clinical severity scores
and the ‘standard’ AI-ML baseline.
Discussion & conclusion: We conducted an exhaustive exploration of AI-ML methods designed for
both ordinal and cost-sensitive classification, motivated by a real-world application domain (clinical
severity prognosis) in which these topics arise naturally.
Our model with the best classification performance exploited successfully the ordering information
of ground truth classes, coping with imbalance and asymmetric costs. However, these ordinal and
cost-sensitive aspects are seldom explored in the literature.
Index Terms— Artificial intelligence, COVID-19, cost-sensitive classification, ordinal
classification, SARS-CoV-2 pneumonia, severity prediction.
<h2 style="color:red;">Jorge Angulo</h2>
		     <h3></h3>
<h2 style="color:red;">Martin Bikandi</h2>
		     <h3></h3>
<h2 style="color:red;">Paula Martín</h2>
		     <h3>Machine Learning for prediction in energy markets
</h3>
		     Initial Margin is a deposit required in financial derivative transactions, such as futures contracts, to support market positions and guarantee compliance with obligations. Given the growing volatility in energy prices and demand, predictive models have been developed in collaboration between the Basque Center for Applied Mathematics (BCAM) and the Finance department of Iberdrola, to determine the Initial Margin in energy markets. The techniques used have made it possible to obtain predictions for 60 products covering electricity and gas in various markets. To do this, recurrent long- and short-term memory (LSTM) neural networks have been used. These neural networks are retrained daily and generate predictions for multiple time horizons. These predictions have proven to be useful, especially in volatile periods and with frequent changes in trends, as demonstrated when evaluating the data for 2022 and 2023.
<h2 style="color:red;">Jose Antonio</h2>
	<h3></h3>
<h2 style="color:red;">Josu Ceberio</h2>
	<h3>Time to stop and think: what kind of research do we want to do?</h3>
		     Experimentation is an intrinsic part of research since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions. In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection process is too complex and extensive to be tackled as a whole. Therefore, to bring our feet to the ground, we will include in this document our reflections about the role of experimentation in the two approaches of conducting research: engineering vs. scientific.
<h2 style="color:red;">Mikel Malagón</h2>
	<h3>Self-Composing Policies for Scalable Continual Reinforcement Learning</h3>
		     We introduce a growable and modular neural network architecture that
naturally avoids catastrophic forgetting and interference in continual
reinforcement learning. Unlike previous growing neural network
approaches, we show that the number of parameters of the proposed
method grows linearly with respect to the number of tasks, and does
not sacrifice plasticity to scale. Experiments conducted in benchmark
continuous control and visual problems reveal that the proposed approach
achieves greater knowledge transfer and performance than alternative methods.
<h2 style="color:red;">Aitor Sanchez</h2>
	<h3>Self-Supervised Anomaly Detection in Time Series: A Brief Introduction.
</h3>
		     Time Series anomaly detection has emerged as a prominent and active research area in recent times. Traditionally, machine learning methods have approached this task from an unsupervised standpoint. These models are trained to learn the normal behavior of data and identify anomalies by quantifying their level of abnormality in the inference phase. However, these approaches often struggle with generalization, as they tend to overfit to the normal data patterns observed during training. Consequently, they fail to effectively detect anomalies in new samples that exhibit slight variations in properties and patterns. To address this challenge, several novel contributions have been made, leveraging self-supervised learning techniques. Self-supervised learning is an unsupervised methodology that seeks to capture the underlying structure of data by predicting what is already known about it. This presentation offers a concise introduction to the application of self-supervised learning-based approaches aimed at enhancing the performance of anomaly detection frameworks for time series data. Additionally, we will introduce a work that is currently in progress.
<h2 style="color:red;">Andoni Irazusta</h2>
	<h3>Leveraging Graph Neural Networks for Combinatorial Optimization
</h3>
		     This presentation delves into the application of Graph Neural Networks (GNNs) for tackling combinatorial optimization problems, prevalent in various domains like logistics, network design, and scheduling. Termed Neural Combinatorial Optimization (NCO), this field leverages the capability of GNNs to encode graph-structured data and discern intricate patterns. Through a case study on the Maximum Cut problem, we will examine diverse NCO applications. Additionally, we will explore the challenges of NCO, such as scalability, generalization, and real-world adoption, and discuss future directions for research.
<h2 style="color:red;">Alexander Olza</h2>
<h3>Domain shift scenarios in Neuroscience: Machine learning approaches to cross-stimuli brain decoding</h3>
Domain shift happens when training and testing data come from different distributions. Regular ML algorithms struggle with this, highlighting the need for Domain Adaptation techniques. Brain decoding predicts cognitive states from brain activity patterns, but open questions in this area suffer from a domain shift that is not always addressed in the literature. Cross-stimuli, cross-subject or cross-device brain decoding are clear examples of that. In this work, we present experimental outcomes from applying several Domain Adaptation methods to  decode brain responses across different stimuli, offering insights into the neuroscience implications of our  findings.


                </p>
            </div>
        </div>
    </div>

    <hr>


<!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; PalencIA 2024</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>




		</body>

</html>
