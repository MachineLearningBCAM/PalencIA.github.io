<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>PalencIA - Location</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="css/clean-blog.min.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body>

    <!-- Navigation -->
    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="index.html">Home</a>
                    </li>
                    <li>
                        <a href="about.html">Location</a>
                    </li>
                    <li>
                        <a href="schedule.html">Schedule</a>
		    </li>
                                       <!--                    <li>
                        <a href="organizers.html">Contact organizers</a>
                    </li>-->
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <!-- Page Header -->
    <!-- Set your background image for this header on the line below. -->
    <header class="intro-header" style="background-image: url('workshop.jpg')">
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <div class="page-heading">
                        <h1>PalencIA 2024</h1>
                        <hr class="small">
                        <span class="subheading">Schedule</span>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- Main Content -->
    <div class="container">
        <div class="row">
		            <div class="col-lg-10 col-lg-offset-2 col-md-10 col-md-offset-1">
               
             <p> 
<pre>
			    <table class="center">
			<tr>
			<th>Day</th>
			<td colspan="4" rowspan="1" class="stage-jupiter" style="font-size: 20px;">Wednesday</td>
			<td colspan="4" rowspan="1" class="stage-jupiter" style="font-size: 20px;">Thursday</td>
			<td colspan="4" rowspan="1" class="stage-jupiter" style="font-size: 20px;">Friday</td>
		</tr>
		<tr>
			<th>08:00</th>
			<td colspan="4" rowspan="2"> </td>
			<td colspan="4" rowspan="2" class="stage-earth" style="font-size: 20px;">Breakfast</td>
			<td colspan="4" rowspan="2" class="stage-earth" style="font-size: 20px;">Breakfast</td>
		</tr>
		<tr>
			<th>08:30</th>
		</tr>
		<tr>
			<th>09:00</th>
			<td colspan="4" rowspan="4"> </td>
			<td colspan="4" rowspan="4" class="stage-mars" style="font-size: 15px;"><a href="#inigo"><b>Iñigo Urteaga</b></a>
<small><small>A Coreset-based, Tempered Variational Posterior for Accurate and 
Scalable Stochastic Gaussian Process Inference</small></small>
<a href="#xabierdejuan"><b>Xabier de Juan</b></a>
<small><small>Optimality of the Median-of-Means for Learning Under Contamination</small></small>
<a href="#mario"><b>Mario Martínez</b></a>
<small><small>Enhancing model performance through privileged information: 
A preliminary approach with probability distributions to harness privileged features</small></small>
<a href="#aitor"><b>Aitor Sánchez</b></a>
<small><small>Self-Supervised Anomaly Detection in Time Series: A Brief Introduction</small></small></td>
			<td colspan="4" rowspan="4" class="stage-mars" style="font-size: 15px;"><a href="#xabierbenavides">Xabier Benavides</a>
<a href="#rodrigo">Rodrigo González</a>
<small><small>Disaggregated short-term forecasting of electricity consumption</small></small>
<a href="#ioar">Ioar Casado</a>
<small><small>PAC-Bayes-Chernoff bounds and applications</small></small>
<a href="#aritz">Aritz Pérez</a></td>
		</tr>
		<tr>
			<th>09:30</th>
		</tr>
		<tr>
			<th>10:00</th>
		</tr>
		<tr>
			<th>10:30</th>
		</tr>
		<tr>
			<th>11:00</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Coffee break</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Coffee break</td>
		</tr>
		<tr>
			<th>11:30</th>
			<td colspan="4" rowspan="4"> </td>
			<td colspan="4" rowspan="4" class="stage-mars" style="font-size: 15px;"><a href="#andoni"><b>Andoni Irazusta</b></a>
<small><small>Leveraging Graph Neural Networks for Combinatorial Optimization</small></small>
<a href="#josu"><b>Josu Ceberio</b></a>
<small><small>Time to stop and think: what kind of research do we want to do?</small></small></td>
			<td colspan="4" rowspan="4" class="stage-mars" style="font-size: 15px;"><a href="#jorge">Jorge Angulo</a>
<small><small>Cost sensitive analysis for MRCs</small></small>
<a href="#mikel"><b>Mikel Malagon</b></a>
<small><small>Self-Composing Policies for Scalable Continual Reinforcement Learning</small></small>
<a href="#martin">Martín Bikandi</a></td>
<small><small>Classification of streaming time series for human activity recognition with 2-D representations</small></small>
		</tr>
		<tr>
			<th>12:00</th>
		</tr>
		<tr>
			<th>12:30</th>
		</tr>
		<tr>
			<th>13:00</th>
		</tr>
		<tr>
			<th>13:30</th>			
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Welcome</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Bus to Cervera</td>
			<td colspan="4" rowspan="5" class="stage-saturn" style="font-size: 20px;">Lunch break<span>Location: Hotel</span></td>
		</tr>
		<tr>
			<th>14:00</th>
			<td colspan="4" rowspan="4" class="stage-saturn" style="font-size: 20px;">Lunch break<span>Location: Ruesga (Booked)</span></td>
			<td colspan="4" rowspan="4" class="stage-saturn" style="font-size: 20px;">Lunch break<span>Location: Cervera (On your own)</span></td>
		</tr>
		<tr>
			<th>14:30</th>
		</tr>
		<tr>
			<th>15:00</th>
		</tr>
		<tr>
			<th>15:30</th>
		</tr>
		<tr>
			<th>16:00</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Check in</td>
			<td colspan="4" rowspan="3" class="stage-mars" style="font-size: 15px;"><a href="#ainhize">Ainhize Barrainkua</a>
<small><small>Fairness without Demographics through Uncertainty Set Modification</small></small>
<a href="#jose">José Ignacio Segovia</a>
<small><small>Double-Weighting Adaptation for Multi-Source Covariate Shift</small></small>
<a href="#kartheek">Kartheek</a>
<small><small>Constraint generation method for efficient learning of minimax risk classifiers</small></small></td>
			<td colspan="4" rowspan="3"> </td>
		</tr>
		<tr>
			<th>16:30</th>
			<td colspan="4" rowspan="2" class="stage-mars" style="font-size: 15px;"><a href="#santi"><b>Santiago Mazuelas</b></a>
<small><small>Minimax Semi-Supervised Learning with Performance Guarantees</small></small>
<a href="#veronica"><b>Verónica Álvarez</b></a>
<small><small>Weak Supervision with Confidence Intervals for Probabilities and General Labeling Functions</small></small>
<a href="#nicolas"><b>Nicolás Errandonea</b></a>
<small><small>Graph based ML models for the partially labeled classification problem</small></small></td>
		</tr>
		<tr>
			<th>17:00</th>
		</tr>
		<tr>
			<th>17:30</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Coffee break</td>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Coffee break</td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>18:00</th>
			<td colspan="4" rowspan="3" class="stage-mars" style="font-size: 15px;"><a href="#onintze"><b>Onintze Zaballa</b></a>
<small><small>Multi-output</small></small>
<a href="#paula"><b>Paula Martín</b></a>
<small><small>Machine Learning for prediction in energy markets</small></small>
<a href="#fernando"><b>Fernando García</b></a>
<small><small>Cost-sensitive ordinal classification methods to predict SARS-CoV-2 
pneumonia severity intelligence machine learning (AI-ML) strategies in 
the prognosis of SARS-CoV-2 pneumonia severity</small></small></td>
			<td colspan="4" rowspan="3" class="stage-mars" style="font-size: 15px;"><a href="#leo">Leo Vattoly</a>
<small><small>Early Time Series Classification</small></small>
<a href="#regisn">Regis Konan</a>
<small><small>Exploring Autoencoders and Variational Autoencoders: 
Theory, Applications, and Future Directions</small></small>
<a href="#alexander">Alexander Olza</a>
<small><small>Domain shift scenarios in Neuroscience: 
Machine learning approaches to cross-stimuli brain decoding</small></small></td>
			<td colspan="4" rowspan="3"> </td>
		</tr>
		<tr>
			<th>18:30</th>
		</tr>
		<tr>
			<th>19:00</th>
		</tr>
		<tr>
			<th>19:30</th>
			<td colspan="4" rowspan="1" class="stage-earth" style="font-size: 20px;">Bus to Cervera</td>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>20:00</th>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
		<tr>
			<th>20:30</th>
			<td colspan="4" rowspan="1" class="stage-saturn" style="font-size: 20px;">Dinner <span>Location: Cervera (On your own)</span></td>
			<td colspan="4" rowspan="1" class="stage-saturn" style="font-size: 20px;">Dinner <span>Location: Hotel</span></td>
			<td colspan="4" rowspan="1"> </td>
		</tr>
      
</table>
</pre>
                </p>
            </div>
        </div>
    </div>

    <hr>

	    <div class="container">
        <div class="row">
                           <div class="col-lg-20 col-lg-offset-2 col-md-20 col-md-offset-1">
             <p> 
<h1 style="color:blue;" >Abstracts </h1>
<a id="santi"><h2 style="color:red;">Santiago Mazuelas</h2></a>
<h3>Minimax Semi-Supervised Learning with Performance Guarantees</h3>

Semi-supervised learning holds promise to significantly increase the cost-efficiency of data acquisition by using a relatively small set of supervised samples. The scarcity of labeled samples poses important methodological challenges, for instance, the limited availability of validation samples hinders the usage of conventional tools to assess predictive performance. Both in theory and practice, the benefits of semi-supervision have been manifested in certain specific situations, in contrast to the broad success of supervised and unsupervised methods. This talk presents semi-supervised minimax risk classifiers (SMRCs) that can leverage the representation and discriminative capabilities of general supervised and unsupervised learning algorithms. In addition, we provide generalization bounds for SMRCs, show how the methods proposed can provide computable performance guarantees, and present efficient learning algorithms based on stochastic subgradients. The experimental results show that SMRCs can effectively leverage general algorithms and provide practically useful performance guarantees.

<a id="onintze"><h2 style="color:red;">Onintze Zaballa</h2></a>
		     <h3></h3>
<a id="veronica"><h2 style="color:red;">Verónica Álvarez</h2></a>
		     <h3>Weak Supervision with Confidence Intervals for Probabilities and General Labeling Functions</h3>
Weak supervision methods aim to obtain probabilistic predictions by leveraging the information provided by multiple weak labeling functions (LFs). General LFs commonly have assorted types and largely unknown interdependences that often result in unreliable probabilistic predictions. In addition, existing techniques cannot assess the predictions’ reliability. This talk presents a weak supervision methodology that encapsulates the information provided by general LFs into an uncertainty set of distributions consistent with the LFs’ observed behavior. Such uncertainty set can then be utilized to assess the reliability of probabilistic predictions in terms of confidence intervals, and to obtain more reliable probabilistic predictions that minimize worst-case expected losses. Experiments on multiple benchmark datasets show the improvement of the presented methods over the state-of-the-art and the reliability of the confidence intervals presented.
<a id="nicolas"><h2 style="color:red;">Nicolás Errandonea</h2></a>
		     <h3>Graph based ML models for the partially labeled classification problem</h3>
Graph based machine learning models have been broadly analyzed and used in the semi-supervised setting. However, the application of graph models to other less explored weakly supervised settings is yet to be explored. In the partially labeled problem,  instances are paired with sets of possible labels that contain the true label.The present work analyzes the possible advantages of applying graph based  models to the partially labeled problem, since the elaboration of a  model  that actively exploits  the underlying connections among the data elements presents itself as a promising  line of research.

<a id="xabierdejuan"><h2 style="color:red;">Xabier de Juan Soriano</h2></a>
		     <h3>Optimality of the Median-of-Means for Learning Under Contamination
</h3>
We present a concentration inequality for the Median-of-Means (MoM) estimator under adversarial corruption which achieves the minimax optimal order under minimal conditions. In addition, we show that the derived bound can be used to obtain generalization bounds for minimax classification rules under adversarial contamination
<a id="jose"><h2 style="color:red;">José Ignacio Segovia</h2></a>
		     <h3>Double-Weighting Adaptation for Multi-Source Covariate Shift
</h3>
In multi-source covariate shift scenarios, the training data is obtained from multiple sources, each of which has different marginal probability distributions over the instances. Most of the existing approaches extend the reweighted covariate shift approach to the multi-source scenario, leading to two-stage weighting methods. Other approaches combine classification rules learnt independently on each source. However, existing methods inherit the limitations of reweighted approaches, do not exploit complementary information among sources, and equally combine sources for all instances. We propose a learning methodology, multi-source double-weighting (MS-DW), based on the double-weighting approach for multi-source covariate shift adaptation. The presented methods assign source-dependent weights for training and testing instances, where weights are obtained jointly using information from all sources. Theoretically, we develop generalization bounds for the proposed methods that show a significant improvement in estimation error by leveraging rich complementary information among sources. Empirically, the proposed methods achieve enhanced classification performance in both synthetic and empirical experiments.
<a id="inigo"><h2 style="color:red;">Iñigo Urteaga</h2></a>
		     <h3>A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference
</h3>
I will present a stochastic variational inference method for Gaussian processes (GPs), which is based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for GPs (CVTGP) is defined in terms of the GP prior and the data-likelihood; hence, accommodating the modeling inductive biases.

CVTGP is amenable to stochastic optimization, it reduces the learnable parameter size, enjoys numerical stability, and maintains state-of-the-art time- and space-complexity.

Results on simulated and real-world regression problems with Gaussian observation noise validate that CVTGP provides better evidence lower-bound estimates and predictive root mean squared error than alternative stochastic GP inference methods.
<a id="ainhize"><h2 style="color:red;">Ainhize Barrainkua</h2></a>
		     <h3>Fairness without Demographics through Uncertainty Set Modification
</h3>
Minimax risk classifiers optimize worst-case accuracy under distributional uncertainty, while being blind to demographics. This implicitly aligns with the underlying goal of minimax fairness, which seeks a classification rule that maximizes worst-group accuracy. The fairness guarantees of minimax risk classifiers depend on the uncertainty set chosen. We propose a method to determine the optimal uncertainty set size that maximizes worst-group accuracy, with partial or no access to sensitive information.  
<a id="regis"><h2 style="color:red;">Regis Konan</h2></a>
		     <h3>Exploring Autoencoders and Variational Autoencoders: Theory, Applications,
and Future Directions.
</h3>
Autoencoders (AEs) and variational autoencoders (VAEs) are fundamental architectures in machine learning, providing powerful tools for data compression, reconstruction, and generation. This presentation aims to explore the theory behind these models, their practical implementation, and future research directions. In the first part, we present the theoretical underpinnings of AEs and VAEs, highlighting their ability to learn efficient representations of data by reducing dimensionality while preserving essential information. We also explore the main differences between the two approaches, in particular the latent distribution constraint imposed by VAEs. The second part focuses on the practical application of AEs and VAEs, with an in-depth study of the impact of network architectures on their performance. We examine how different network structures affect the ability to reconstruct and generate models, thus providing valuable information for the design of robust machine learning systems. We then consider an in-depth analysis of the impact of learning and optimization parameters and the impact of the regularization parameter β on VAE loss and investigate the impact of VAE priors. Finally, in the third part, we discuss future developments and challenges in this domain.
<a id="leo"><h2 style="color:red;">Leo Vattoly</h2></a>
		     <h3> Early Time Series Classification For Polymer Systems
</h3>
My work is centred on the application of Machine Learning (ML) techniques to polymer systems. We are currently in the process of developing algorithms utilizing data obtained from a mathematical model that simulates polymer reactions. This simulator is capable of producing time series data that corresponds to the morphology of polymer particles (structure of polymer particles). As an ML developer, my primary responsibility involves classifying these reactions based on their anticipated morphology. If the reactor conditions fail to produce the expected structure by the conclusion of the reaction, it is imperative to promptly cease the process to prevent the wastage of chemical resources. I am utilizing an Early Time Series Classification (ETSC) methodology to address this issue. Our objective is to minimize the number of timestamps in the data while maximizing the accuracy of the classification model.
<a id="kartheek"><h2 style="color:red;">Kartheek</h2></a>
		     <h3>Constraint generation method for efficient learning of minimax risk classifiers
</h3>
High-dimensional data is common in multiple areas, such as health care and genomics, where the number of features can be hundreds of thousands. In addition, learning methods often perform a high-dimensional representation of the input data vector in order to improve the classification performance such as self supervised approach. In such scenarios, the large number of features lead to inefficient learning. This talk will present methods based on constraint generation to enable efficient learning of minimax risk classifiers (MRCs). In particular, I will discuss methods efficient in high dimensions and its extension to settings with a large number of samples (large number of features in dual). 
<a id="xabierbenavides"><h2 style="color:red;">Xabier Benavides</h2></a>
		     <h3></h3>
<a id="mario"><h2 style="color:red;">Mario Martinez</h2></a>
		     <h3>Enhancing model performance through privileged information: A preliminary approach with probability distributions to harness privileged features.
</h3>
The paradigm of privileged information leverages privileged features present at training time, but not at prediction, as additional information for training the models. A preliminary approach for learning with privileged information is presented. Firstly, a probability distribution for the privileged feature is generated for each instance using a neural network and regular features (available at training and prediction time) as inputs. Secondly, alternatives are proposed to exploit the probability distributions and improve the performance of the model trained with regular features.
<a id="rodrigo"><h2 style="color:red;">Rodrigo Gonzalez</h2></a>
		     <h3>Disaggregated short-term forecasting of electricity consumption</h3>
Obtaining accurate forecasts of electricity consumption is key to promoting efficiency, reducing waste and adapting energy production to the needs of the ecological transition. The increasing availability of data at a higher level of detail has allowed the realization of disaggregated predictions that, by segmenting the data into different profiles and making predictions on these profiles, are better adapted to the data. In this work we propose to group the observations into profiles with homogeneous consumption patterns and thus perform the prediction by training the model in a disaggregated way. Thus, we evaluate the improvements of predicting in a disaggregated way with respect to doing it in an aggregated way, as well as the differences between different clustering approaches, with special emphasis on the use of the k-means algorithm. We see how disaggregated prediction significantly improves the accuracy within each subset.
<a id="ioar"><h2 style="color:red;">Ioar Casado</h2></a>
		     <h3>PAC-Bayes-Chernoff bounds and applications</h3>
We introduce a new PAC-Bayes oracle bound for unbounded losses. This result can be understood as a PAC-Bayesian version of the Cramér-Chernoff bound. The proof technique relies on controlling the tails of certain random variables involving the Cramér transform of the loss. We highlight several applications of the main theorem. First, we show that our result naturally allows exact optimization of the free parameter on many PAC-Bayes bounds. Second, we recover and generalize previous results. Finally, we show that our approach allows working with richer assumptions that result in more informative and potentially tighter bounds. In this direction, we provide a general bound under a new ``model-dependent bounded CGF" assumption from which we obtain bounds based on parameter norms and log-Sobolev inequalities. All these bounds can be minimized to obtain novel posteriors. 

<a id="aritz"><h2 style="color:red;">Aritz Pérez</h2></a>
		     <h3></h3>
<a id="fernando"><h2 style="color:red;">Fernando Garcia</h2></a>
		     <h3>Cost-sensitive ordinal classification methods to predict SARS-CoV-2 pneumonia severity
intelligence machine learning (AI-ML) strategies in the prognosis of SARS-CoV-2 pneumonia severity.
</h3>
Materials & methods: Observational, retrospective, longitudinal, cohort study in 4 hospitals in Spain. Information regarding demographic and clinical status was supplemented by socioeconomic data and air pollution exposures. We proposed AI-ML algorithms for ordinal classification via ordinal decomposition and for costsensitive learning via resampling techniques. For performance-based model selection, we defined a custom score including per-class sensitivities and asymmetric misprognosis costs. 260 distinct AIML models were evaluated via 10 repetitions of 5Å~5 nested cross-validation with hyperparameter tuning. Model selection was followed by the calibration of predicted probabilities. Final overall performance was compared against five well-established clinical severity scores and against a ‘standard’ (non-cost sensitive, non-ordinal) AI-ML baseline. In our best model, we also evaluated its explainability with respect to each of the input variables. 
Results: The study enrolled n=1548 patients: 712 experienced low, 238 medium, and 598 high clinical severity. d=131 variables were collected, becoming d′=148 features after categorical encoding. Model selection resulted in our best-performing AI-ML pipeline having: a) no imputation of missing data, b) no feature selection (i.e. using the full set of d′ features), c) ‘Ordered Partitions’ ordinal decomposition, d) cost-based reimbalance, and e) a Histogram-based Gradient Boosting classifier. This best model (calibrated) obtained a median accuracy of 68.1% [67.3%, 68.8%] (95% confidence interval), a balanced accuracy of 57.0% [55.6%, 57.9%], and an overall area under the curve (AUC) 0.802 [0.795, 0.808]. In our dataset, it outperformed all five clinical severity scores and the ‘standard’ AI-ML baseline. Discussion & conclusion: We conducted an exhaustive exploration of AI-ML methods designed for both ordinal and cost-sensitive classification, motivated by a real-world application domain (clinical severity prognosis) in which these topics arise naturally. Our model with the best classification performance exploited successfully the ordering information of ground truth classes, coping with imbalance and asymmetric costs. However, these ordinal and cost-sensitive aspects are seldom explored in the literature. Index Terms— Artificial intelligence, COVID-19, cost-sensitive classification, ordinal classification, SARS-CoV-2 pneumonia, severity prediction.
<a id="jorge"><h2 style="color:red;">Jorge Angulo</h2></a>
		     <h3>Cost sensitive analysis for MRCs</h3>
In supervised classification, the problem is often formulated using 0-1 loss or, more often, a surrogate loss. However, for many applications and problem formulations, some problem formulations require other types of losses. Amongst this we have abstention voting (where we introduce a new class label), ordinal classification or training with some unlabeled data (missing values). The goal of this article is to extend the minimax risk classifiers (MCR) to work with an arbitrary loss matrix of size n1 ×n2.
<a id="martin"><h2 style="color:red;">Martín Bikandi</h2></a>
		     <h3>Classification of streaming time series for human activity recognition with 2-D representations</h3>
Wearable devices such as smartwatches or pocket-carried smartphones can generate sensor information continuously. This type of information, because of its online nature, must be efficiently processed due to constraints in storage and compute capacity, time-dependent data streams are also called Streaming Time Series. This kind of data often comes with problems such as class imbalance, anomalies and drift among others. Research on offline time series classification is abundant, while the online case has received less attention. We propose a method based on online elastic similarity measures (O-DTW) to increasingly compute a 2-D representation of the time series, and classify incoming points using a CNN. Additionally, we test other image-based representations for time series on three real-world HAR datasets to compare our methods.
<a id="paula"><h2 style="color:red;">Paula Martín</h2></a>
		     <h3>Machine Learning for prediction in energy markets
</h3>
		     Initial Margin is a deposit required in financial derivative transactions, such as futures contracts, to support market positions and guarantee compliance with obligations. Given the growing volatility in energy prices and demand, predictive models have been developed in collaboration between the Basque Center for Applied Mathematics (BCAM) and the Finance department of Iberdrola, to determine the Initial Margin in energy markets. The techniques used have made it possible to obtain predictions for 60 products covering electricity and gas in various markets. To do this, recurrent long- and short-term memory (LSTM) neural networks have been used. These neural networks are retrained daily and generate predictions for multiple time horizons. These predictions have proven to be useful, especially in volatile periods and with frequent changes in trends, as demonstrated when evaluating the data for 2022 and 2023.
<a id="josu"><h2 style="color:red;">Josu Ceberio</h2></a>
	<h3>Time to stop and think: what kind of research do we want to do?</h3>
		     Experimentation is an intrinsic part of research since it allows for collecting quantitative observations, validating hypotheses, and providing evidence for their reformulation. For that reason, experimentation must be coherent with the purposes of the research, properly addressing the relevant questions in each case. Unfortunately, the literature is full of works whose experimentation is neither rigorous nor convincing, oftentimes designed to support prior beliefs rather than answering the relevant research questions. In this paper, we focus on the field of metaheuristic optimization, since it is our main field of work, and it is where we have observed the misconduct that has motivated this letter. Our main goal is to sew the seed of sincere critical assessment of our work, sparking a reflection process both at the individual and the community level. Such a reflection process is too complex and extensive to be tackled as a whole. Therefore, to bring our feet to the ground, we will include in this document our reflections about the role of experimentation in the two approaches of conducting research: engineering vs. scientific.
<a id="mikel"><h2 style="color:red;">Mikel Malagón</h2></a>
	<h3>Self-Composing Policies for Scalable Continual Reinforcement Learning</h3>
We introduce a growable and modular neural network architecture that naturally  voids catastrophic forgetting and interference in continual reinforcement learning. Unlike previous growing neural network approaches, we show that the number of parameters of the proposed method grows linearly with respect to the number of tasks, and does not sacrifice plasticity to scale. Experiments conducted in benchmark continuous control and visual problems reveal that the proposed approach achieves greater knowledge transfer and performance than alternative methods.
<a id="aitor"><h2 style="color:red;">Aitor Sánchez</h2></a>
	<h3>Self-Supervised Anomaly Detection in Time Series: A Brief Introduction.
</h3>
Time Series anomaly detection has emerged as a prominent and active research area in recent times. Traditionally, machine learning methods have approached this task from an unsupervised standpoint. These models are trained to learn the normal behavior of data and identify anomalies by quantifying their level of abnormality in the inference phase. However, these approaches often struggle with generalization, as they tend to overfit to the normal data patterns observed during training. Consequently, they fail to effectively detect anomalies in new samples that exhibit slight variations in properties and patterns. To address this challenge, several novel contributions have been made, leveraging self-supervised learning techniques. Self-supervised learning is an unsupervised methodology that seeks to capture the underlying structure of data by predicting what is already known about it. This presentation offers a concise introduction to the application of self-supervised learning-based approaches aimed at enhancing the performance of anomaly detection frameworks for time series data. Additionally, we will introduce a work that is currently in progress.
<a id="andoni"><h2 style="color:red;">Andoni Irazusta</h2></a>
	<h3>Leveraging Graph Neural Networks for Combinatorial Optimization
</h3>
		     This presentation delves into the application of Graph Neural Networks (GNNs) for tackling combinatorial optimization problems, prevalent in various domains like logistics, network design, and scheduling. Termed Neural Combinatorial Optimization (NCO), this field leverages the capability of GNNs to encode graph-structured data and discern intricate patterns. Through a case study on the Maximum Cut problem, we will examine diverse NCO applications. Additionally, we will explore the challenges of NCO, such as scalability, generalization, and real-world adoption, and discuss future directions for research.
<a id="alexander"><h2 style="color:red;">Alexander Olza</h2></a>
<h3>Domain shift scenarios in Neuroscience: Machine learning approaches to cross-stimuli brain decoding</h3>
Domain shift happens when training and testing data come from different distributions. Regular ML algorithms struggle with this, highlighting the need for Domain Adaptation techniques. Brain decoding predicts cognitive states from brain activity patterns, but open questions in this area suffer from a domain shift that is not always addressed in the literature. Cross-stimuli, cross-subject or cross-device brain decoding are clear examples of that. In this work, we present experimental outcomes from applying several Domain Adaptation methods to  decode brain responses across different stimuli, offering insights into the neuroscience implications of our  findings.


                </p>
            </div>
        </div>
    </div>

    <hr>


<!-- Footer -->
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                    <ul class="list-inline text-center">
                    </ul>
                    <p class="copyright text-muted">Copyright &copy; PalencIA 2024</p>
                </div>
            </div>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="js/clean-blog.min.js"></script>




		</body>

</html>
